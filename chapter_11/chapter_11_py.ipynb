{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c33f29c-46e0-4328-beda-8f758684b57d",
   "metadata": {},
   "source": [
    "# The StatQuest Illustrated Guide to Statistics\n",
    "## Chapter 11 - Using Regression to Test for More Differences with ANOVA and ANCOVA!!!\n",
    "\n",
    "Copyright 2026, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356cead-21bc-4a79-bfcc-972787453cab",
   "metadata": {},
   "source": [
    "In this notebook we'll learn how to...\n",
    "\n",
    "- Perform ANOVA tests using `ols()` and `anova_lm()`.\n",
    "- Perform post-hoc tests to identify specific differences.\n",
    "- Perform an ANCOVA test.\n",
    "- Calculate *p*-values for ANOVA and ANCOVA using the null hypothesis to build a histogram.\n",
    "\n",
    "**NOTE:**\n",
    "This tutorial assumes that you have installed **[Python](https://www.python.org/)** and read Chapter 11 in **[The StatQuest Illustrated Guide to Statistics]()**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33ef62-26e0-4c72-a370-3e757ac9260e",
   "metadata": {},
   "source": [
    "Since we're using Python, the first thing we do is load in some modules that will help us do math and plot graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df8779-919a-4c42-8909-184e84465d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import seaborn as sns # to draw a graphs and have them look somewhat nice\n",
    "import matplotlib.pyplot as plt # give us easy control of the range of values for\n",
    "                                # the x and y axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bb06c-982b-4e96-a3dd-bea67cb469f5",
   "metadata": {},
   "source": [
    "# Performing ANOVA tests using `ols()` and `anova_lm()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c8301-69b3-4bad-adff-e9085e74d424",
   "metadata": {},
   "source": [
    "If we're going to do ANOVA, then the first thing we need is some data. In this example, we'll use the dataset illustrated in Chapter 11, which has the recovery times measured for 4 different drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e669f8e-24ec-4c78-94fc-174d7f47a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, create the data\n",
    "drug_a = [8, 12, 22]\n",
    "drug_b = [20, 29, 39]\n",
    "drug_c = [6, 17, 20]\n",
    "drug_d = [45, 39, 30]\n",
    "\n",
    "recovery_time = drug_a + drug_b + drug_c + drug_d\n",
    "recovery_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5416f-4cc5-4034-96a8-809743f98d39",
   "metadata": {},
   "source": [
    "Now let's put `recovery_time` into a `data_frame` that uses a factor, `drug`, to organize each measurement. First, let's create the factor, `drug`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a7b9d-8be2-4ff1-8024-6a248a04e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a \"factor\" variable that\n",
    "## allows us to keep track of which drug\n",
    "## is associated with which time in recovery_time\n",
    "drug = (['a'] * 3) + (['b'] * 3) + (['c'] * 3) + (['d'] * 3)\n",
    "\n",
    "## print out the values\n",
    "drug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e47511-b493-4211-a408-c272a5243351",
   "metadata": {},
   "source": [
    "...now let's add `drug` to a `DataFrame` with `recovery_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695cca0-052d-46e7-8d4a-f03e41d22047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'time': recovery_time,\n",
    "    'drug': pd.Categorical(drug)\n",
    "})\n",
    "\n",
    "## print it out\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edf434-4d51-4add-a98d-97bb941e721c",
   "metadata": {},
   "source": [
    "Now, just like we did for Simple Linear Regression, Multiple Regression, and *t*-tests, we can call the `ols()` and `summary()` functions with the `DataFrame` and a simle formula, `time ~ drug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591cf35-486b-4c77-bbcd-16d6bfe3bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using OLS for ANOVA\n",
    "model = smf.ols('time ~ drug', data=df)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79208e3d-1bd1-401c-879b-5e9496c18af6",
   "metadata": {},
   "source": [
    "The *p*-value, `Prob (F-statistic):\t0.0149`, tells us to reject the Null Hypothesis that there are no differences in recovery time among the 4 drugs.\n",
    "\n",
    "Now let's see what happens when, instead of calling `summary`, we use the `anova_lm()` function, which is specialized for summarizing ANOVA, with the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a03f4-5c6d-454b-bc8d-aa0ef783ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33a403-54df-433f-a08d-7b13cd97468e",
   "metadata": {},
   "source": [
    "And we see that the `anova_lm()` function simplifies the output significantly, but still gives us the same *F*-value and the same *p*-value, `PR(>F) = 0.014867`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008dd42d-9596-4dbc-b75f-76a897c34b34",
   "metadata": {},
   "source": [
    "Now let's see how we can generate the same *p*-value using the histogram method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e578c6-4009-47af-a852-0e64c73fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## since we're going to generate random datasets,\n",
    "## let's start by setting the seed so that the results\n",
    "## are reproducable\n",
    "np.random.seed(0)\n",
    "\n",
    "## To generate random datasets, we'll use a\n",
    "## normal distribution. This distributions\n",
    "## will be based on our observed data, so we\n",
    "## we need to calculate the estimated\n",
    "## mean and standard deviation.\n",
    "overall_mean = df['time'].mean()\n",
    "overall_sd = df['time'].std()\n",
    "\n",
    "## Next, we define the number of random\n",
    "## datasets we wantt o create...\n",
    "num_rand_datasets = 10_000\n",
    "\n",
    "## ...and we define the number of data points\n",
    "## per dataset\n",
    "num_datapoints = len(drug_a)\n",
    "\n",
    "## Create array to store R-squared values\n",
    "rand_r_squared = np.empty(num_rand_datasets)\n",
    "\n",
    "## Here is the loop were we create a bunch of random datasets,\n",
    "## each with num.datapoints values, do an ANOVA\n",
    "## with the random data, then calculate and store\n",
    "## the R-squared values\n",
    "for i in range(num_rand_datasets):\n",
    "    \n",
    "    ## generate random values for each drug\n",
    "    \n",
    "    r_a = np.random.normal(loc=overall_mean, scale=overall_sd, size=num_datapoints)\n",
    "    r_b = np.random.normal(loc=overall_mean, scale=overall_sd, size=num_datapoints)\n",
    "    r_c = np.random.normal(loc=overall_mean, scale=overall_sd, size=num_datapoints)\n",
    "    r_d = np.random.normal(loc=overall_mean, scale=overall_sd, size=num_datapoints)\n",
    "\n",
    "    ## bundle the random values together in a DataFrame\n",
    "    \n",
    "    recovery_time_rand = np.concatenate([r_a, r_b, r_c, r_d])\n",
    "    \n",
    "    drug_rand = ((['drug.a'] * num_datapoints) +\n",
    "                 (['drug.b'] * num_datapoints) +\n",
    "                 (['drug.c'] * num_datapoints) +\n",
    "                 (['drug.d'] * num_datapoints))\n",
    "    \n",
    "    data_rand = pd.DataFrame({'time': recovery_time_rand, \n",
    "                              'drug': pd.Categorical(drug_rand)})\n",
    "\n",
    "    ## fit a linear regression line to the random data\n",
    "    ## and calculate R-squared\n",
    "    model = smf.ols('time ~ drug', data=data_rand)\n",
    "    model_results = model.fit()\n",
    "    rand_r_squared[i] = model_results.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc08a-be17-4c6e-86e9-0808f5a26679",
   "metadata": {},
   "source": [
    "Now let's draw a histogram of the $R^2$ values with the `histplot()` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23decdae-5eff-49db-91b4-94da56e5a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=rand_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85729915-1da2-4993-a4b7-a0949848383f",
   "metadata": {},
   "source": [
    "...and calculate the *p*-value as the precentage of \"random\" $R^2$ values greater than or equal to the one we got for our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096bf9-f251-4c53-9d06-65ffbcd9a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of randomly generated rsquared >= the original rsquared\n",
    "num_greater = np.sum(rand_r_squared >= results.rsquared)\n",
    "\n",
    "# calculate the p-value \n",
    "p_value = num_greater / num_rand_datasets\n",
    "\n",
    "# print out the p-value\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c76d82-7ba1-4eb2-8c79-7b0011ed2843",
   "metadata": {},
   "source": [
    "Thus, the *p*-value calculated with the histogram is 0.0146. Now let's compare that to the *p*-value stored in `anova.summary`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e7216-334e-4bdd-a30b-a89ab67e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe7fb0-85c4-4979-aaf0-080007b16f7e",
   "metadata": {},
   "source": [
    "So, at last, we see that the two *p*-values are essentially the same.\n",
    "\n",
    "# BAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf0376-2e99-40b4-9ba8-d204d14b6b06",
   "metadata": {},
   "source": [
    "Now that we have relatively high confidence that there are differences among the four drugs, let's do all pairwise *t*-tests so that we can identify which drugs are different. These tests, done after an ANOVA to identify the indiviual groups that are different, are called **Post-Hoc** tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519831e5-7ccd-4f00-a925-b2fe7c9903dc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6ad54-931d-4d0a-a21f-51795dd154dc",
   "metadata": {},
   "source": [
    "Performaing all pairwise *t*-tests in Python is super easy with the `posthoc_ttest()` function. Not only will this function do all of the pairwise *t*-tests for us, it will also adjust the *p*-values using any of the methods that we pass to the `multipletests()` function. We'll start by using the Holm correction for multiple testing.\n",
    "\n",
    "NOTE: `posthoc_ttest()` is part of the `scikit_posthocs` package, and might not be installed for your system. This next chunk of code will install it if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b662e-3224-4897-9447-27ed8c8eab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %%capture prevents this cell from printing a ton of STDERR stuff to the screen\n",
    "\n",
    "## First, check to see if scikit_posthocs is installed, if not, install it.\n",
    "##\n",
    "## NOTE: If you **do** need to install something, just know that you may need to\n",
    "##       restart your session for python to find the new module(s).\n",
    "##\n",
    "##       To restart your session:\n",
    "##       - In Google Colab, click on the \"Runtime\" menu and select\n",
    "##         \"Restart Session\" from the pulldown menu\n",
    "##       - In a local jupyter notebook, click on the \"Kernel\" menu and select\n",
    "##         \"Restart Kernel\" from the pulldown menu\n",
    "import pip\n",
    "try:\n",
    "  __import__(\"scikit_posthocs\")\n",
    "except ImportError:\n",
    "  pip.main(['install', \"scikit_posthocs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae60e7-81bf-44bc-8685-eb588e11986c",
   "metadata": {},
   "source": [
    "Now import `scikit_posthocs`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4d8f0-5bd2-4dfb-a7af-a2dcd80d7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74af543-4583-4398-9a33-3c34bad1a534",
   "metadata": {},
   "source": [
    "...and now we can run `posthoc_ttest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb310f-a320-4335-8e8c-665c369cc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.posthoc_ttest(df, ## the DataFrame that has our data\n",
    "                 val_col='time', ## column with the values\n",
    "                 group_col='drug', ## column that defines how the values should be grouped\n",
    "                 pool_sd=True, ## pool all data to estimate standard deviation\n",
    "                 equal_var=True, ## assume variances are the same for each group.\n",
    "                 p_adjust='holm') ## multiple testing correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807a273-8b87-432a-87c4-aaf0eb8b91b3",
   "metadata": {},
   "source": [
    "So, the output from `posthoc_ttest()` is a matrix of adjusted *p*-values. The first column are all the adjusted *p*-values associated with Drug A, starting with itself (Drug A) at the top and ending with Drug D at the bottom. Likewise, the second colunn gives us the adjusted *p*-values associated with Drug B , the third column gives us the adjusted *p*-values associated with Drug C, and the final column gives us the adjusted *p*-values associated with Drug D.\n",
    "\n",
    "The last row has the two adjusted *p*-values less than 0.05 and both of them are associated with Drug D. Drug D appears to be significantly different from Drug A (adjusted *p*-value = 0.037) and Drug C (adjusted *p*-value = 0.037).\n",
    "\n",
    "Now let's see what happens when we use False Discovery Rates (FDR) to adjust the *p*-values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ad970-af5a-4c1c-9a01-f3ccd3a5e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.posthoc_ttest(df, ## the DataFrame that has our data\n",
    "                 val_col='time', ## column with the values\n",
    "                 group_col='drug', ## column that defines how the values should be grouped\n",
    "                 pool_sd=True, ## pool all data to estimate standard deviation\n",
    "                 equal_var=True, ## assume variances are the same for each group.\n",
    "                 p_adjust='fdr_bh') ## multiple testing correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37eb914-5fff-4122-beda-6d8e4d189c87",
   "metadata": {},
   "source": [
    "...and the conclusions are the same as before. Drug D is significantly different from Drugs A and C. However, with FDR, the adjusted *p*-values tend to be a little smaller.\n",
    "\n",
    "# Double BAM!!!\n",
    "\n",
    "Now let's learn how to do an ANCOVA test in R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf52f4-8b93-4279-a927-4c21bef9259c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370e6a4-3394-4541-a02e-1985330392a8",
   "metadata": {},
   "source": [
    "# Performing an ANCOVA test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b24148-9829-43cb-82d7-28fa89e04394",
   "metadata": {},
   "source": [
    "As always, we start by creating our data. In this case, we'll use the data illustrated in Chapter 11, which has recovery times for two drugs, A and B, as well as the Height of each person in the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f9308-c4b4-4663-befc-ac08847d20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New data for drug A and B\n",
    "drug_a_time = [6, 9, 12.5, 14]\n",
    "drug_a_height = [4, 7.5, 10, 12.5]\n",
    "\n",
    "drug_b_time = [4, 8, 9, 11]\n",
    "drug_b_height = [5, 11, 12.5, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8351a8f-e3bb-44b8-ad5e-c4462abcc703",
   "metadata": {},
   "source": [
    "Now let's package the data we have for the two drugs into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a5c7a-c7c0-403c-a218-0a3d63ded82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a factor to keep track of which\n",
    "## recovery time pairs with which drug.\n",
    "drug = (['Drug A'] * 4) + (['Drug B'] * 4)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'time': drug_a_time + drug_b_time,\n",
    "    'drug': pd.Categorical(drug),\n",
    "    'height': drug_a_height + drug_b_height\n",
    "})\n",
    "\n",
    "## print out the data.frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298061a-ca4e-4474-baab-bf5dfcacccae",
   "metadata": {},
   "source": [
    "Now let's create a plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d330db-b73f-4deb-9826-3dd5d4e9ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"Drug A\": \"#A5D3FC\", \"Drug B\": \"#FF968D\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b5f3e-339f-459f-8390-6ccc0353f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot= sns.scatterplot(x='height', \n",
    "                         y='time', \n",
    "                         data=df,\n",
    "                         hue='drug', ## set the color based on the drug\n",
    "                         s=100,\n",
    "                         palette=colors,\n",
    "                         edgecolor=\"black\")\n",
    "\n",
    "## Ensure that the legend it outside of the graph...\n",
    "sns.move_legend(\n",
    "    my_plot,\n",
    "    \"upper left\", # Location within the anchor box\n",
    "    bbox_to_anchor=(1, 1), # Coordinates (x, y) for the anchor box\n",
    "    title=\"Drug\"\n",
    ")\n",
    "\n",
    "## define the range of values for x and y axes.\n",
    "plt.xlim(0, 16)\n",
    "plt.ylim(0, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee003f-e097-4509-a97b-3118e90473bf",
   "metadata": {},
   "source": [
    "Now, if we ignore the Height data, we can do a *t*-test on the two drugs, with respect to the recovery times, using the the `stats.ttest_ind()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19381ea-a141-418e-aafa-653a3b050e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(drug_a_time, drug_b_time, equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d02fe-8baf-4a2c-9ed7-e06329818620",
   "metadata": {},
   "source": [
    "The resulting *p*-value, 0.3458, is well above 0.05, so, without the Height data, we can't reject the Null Hypothesis and be confident that the two drugs are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a5111-b42f-42f3-8135-f7cc9684731d",
   "metadata": {},
   "source": [
    "Now, just for fun, let's see what happens if we ignore the Drug and just use Height to predict recovery time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21337590-66c3-4074-b2ac-a3236cbee966",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = smf.ols('time ~ height', data=df)\n",
    "results_simple = model_simple.fit()\n",
    "results_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b792c03-99a3-4470-92a8-beed33968d65",
   "metadata": {},
   "source": [
    "The p-value is less than 0.05, so we can reject the Null Hypothesis that using Height to predict Time is no different from using the average Time. However, this doesn't tell us if one drug is different from the other. So we need to combine both Height and Drug in a single model. This can be done by adding both column names, separated by a `+` to the **Formula** that we pass to `ols()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0fbb94-e827-4a9b-a9e3-70228f424552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This only compares fancy to overall mean\n",
    "model_fancy = smf.ols('time ~ height + drug', data=df)\n",
    "results_fancy = model_fancy.fit()\n",
    "results_fancy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d3186-98c7-4ea5-89d8-0718bee52b49",
   "metadata": {},
   "source": [
    "And when we combine both bits of information, the Drug and Height, we can predict Recovery Time much better than just using the average Recovery Time (*p*-value = 0.000184). But the Recovery Time predictions were also much better when we only used Height. So now the question is...Is using Drug and Height to predict Recovery Time better than just using Height?\n",
    "\n",
    "One hint at the answer to this question is to compare the Adjuste R-squared values for both tests. When we compare the Adjusted R-squared values, 0.955 for Drug + Height, compared to 0.477 for just Height, we see that Adjusted R-squared goes way up, even when we add a variable to the model. The increase in Adjusted R-squared suggests that using both variables, Drug and Height, to predict Recovery Time is much better than just using Height. However, we can also calculate a *p*-value to compare the two models directly with the equation for *F*...\n",
    "\n",
    "<span style=\"font-size: 24px;\">\n",
    "$F = \\frac{[\\textrm{SSR(simpler)} - \\textrm{SSR(fancier)}] / (p_\\textrm{fancier} - p_\\textrm{simpler})}\n",
    "    {\\textrm{SSR(fancier)} / (n - p_\\textrm{fancier})}$\n",
    "</span>\n",
    "\n",
    "...and then using *F* to calculate the area under the curve of a suitable *F*-distribution.\n",
    "\n",
    "First, let's look at the Sum of the Squared Residuals for the simpler model that only uses Height to predict Recovery Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71422f85-364c-477c-aacd-6fc0b07525fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_simple.ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f1095-c3b1-4943-9122-d3b056aa1d49",
   "metadata": {},
   "source": [
    "Now let's look at the Sum of the Squared Resiudals for the fancier model that uses Height and Drug to predict Recovery Times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071045e-7543-4ee7-ac74-30fc6d965db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fancy.ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1154e-b890-42d5-9a3e-0c4c76e5118a",
   "metadata": {},
   "source": [
    "Now that we have SSR(simpler) and SSR(fancier), we can calculate *F*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06262836-c4d2-44a2-8483-ceacfc2b3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = 1 # p_fancier - p_simpler = 3 - 2 = 1\n",
    "df2 = 5 # n - p_fancier = 8 - 3 = 5\n",
    "\n",
    "F = ((results_simple.ssr - results_fancy.ssr) / df1) / (results_fancy.ssr / df2)\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05ca5d-a467-4be7-a94c-0fd95595d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, import 'f'\n",
    "from scipy.stats import f\n",
    "\n",
    "## p-value with f-distribution\n",
    "1-f.cdf(x=F, dfn=df1, dfd=df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bb1bd-b565-41ed-8a77-77db6690b649",
   "metadata": {},
   "source": [
    "And the resulting *p*-value tells us that using Drug and Height to predict Recovery Time is significantly different from just using Height to prediction Recovery Time. And the Adjusted R-squared we calculated earlier for the model that uses Drug and Height, 0.955, tells us that this model is much better.\n",
    "\n",
    "Now let's see how we can calculate the *p*-value using the Histogram method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b53b32-80cf-481c-b9dc-1b7bc2e3c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## since we're going to generate random datasets,\n",
    "## let's start by setting the seed so that the results\n",
    "## are reproducable\n",
    "np.random.seed(0)\n",
    "\n",
    "## To generate random datasets, we'll use a\n",
    "## normal distribution. This distributions\n",
    "## will be based on our observed data, so we\n",
    "## we need to calculate the estimated\n",
    "## mean and standard deviation.\n",
    "mean_time = df['time'].mean()\n",
    "sd_time = df['time'].std()\n",
    "\n",
    "mean_height = df['height'].mean()\n",
    "sd_height = df['height'].std()\n",
    "\n",
    "## Next, we define the number of random\n",
    "## datasets we wantt o create...\n",
    "num_rand_datasets = 10_000\n",
    "\n",
    "## ...and we define the number of data points\n",
    "## per dataset\n",
    "num_datapoints = len(drug_a_time)\n",
    "\n",
    "## Create array to store R-squared values\n",
    "rand_r_squared = np.empty(num_rand_datasets)\n",
    "\n",
    "## Here is the loop were we create a bunch of random datasets,\n",
    "## each with num_datapoints values, fit regressions to them, \n",
    "## then calculate and store the R-squared values to compare\n",
    "## the residuals\n",
    "for i in range(num_rand_datasets):\n",
    "    \n",
    "    ## generate random values for each variable\n",
    "    \n",
    "    r_a_time = np.random.normal(loc=mean_time, \n",
    "                                scale=sd_time, \n",
    "                                size=num_datapoints)\n",
    "    r_a_height = np.random.normal(loc=mean_height, \n",
    "                                  scale=sd_height, \n",
    "                                  size=num_datapoints)\n",
    "    \n",
    "    r_b_time = np.random.normal(loc=mean_time, \n",
    "                                scale=sd_time, \n",
    "                                size=num_datapoints)\n",
    "    r_b_height = np.random.normal(loc=mean_height, \n",
    "                                  scale=sd_height, \n",
    "                                  size=num_datapoints)\n",
    "\n",
    "    ## bundle the random values together in a DataFrame\n",
    "    \n",
    "    r_time = np.concatenate([r_a_time, r_b_time])\n",
    "    r_height = np.concatenate([r_a_height, r_b_height])\n",
    "    \n",
    "    drug_rand = ((['Drug A'] * num_datapoints) +\n",
    "                 (['Drug B'] * num_datapoints))\n",
    "    \n",
    "    data_rand = pd.DataFrame({'time': r_time, \n",
    "                              'drug': pd.Categorical(drug_rand),\n",
    "                              'height': r_height})\n",
    "\n",
    "    simple_model = smf.ols('time ~ height', data=data_rand)\n",
    "    simple_results = simple_model.fit()\n",
    "    simple_ssr = simple_results.ssr\n",
    "    # print(\"simple_ssr: \" +  str(simple_ssr))\n",
    "\n",
    "\n",
    "    fancy_model = smf.ols('time ~ height + drug', data=data_rand)\n",
    "    fancy_results = fancy_model.fit()\n",
    "    fancy_ssr = fancy_results.ssr\n",
    "    # print(\"fancy_ssr: \" + str(fancy_ssr))\n",
    "\n",
    "    ## calculate and save the R-squared value\n",
    "    rand_r_squared[i] = (simple_ssr - fancy_ssr) / simple_ssr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4a4d7-4e1c-439c-af49-091411a85442",
   "metadata": {},
   "source": [
    "Now let's draw a histogram of the $R^2$ values with the `histplot()` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29558dc0-01b3-40fc-8b94-dd685699c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=rand_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc18d6-b855-4859-9625-0027af6db19c",
   "metadata": {},
   "source": [
    "...and calculate the *p*-value as the precentage of \"random\" $R^2$ values greater than or equal to the one we got for our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bbefd-8b8e-47cd-9262-eeeeadc1be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of randomly generated rsquared >= the original rsquared\n",
    "num_greater = np.sum(rand_r_squared >= results_fancy.rsquared)\n",
    "\n",
    "# calculate the p-value \n",
    "p_value = num_greater / num_rand_datasets\n",
    "\n",
    "# print out the p-value\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f920a8-e8da-48f1-84e2-94fbbac7ba31",
   "metadata": {},
   "source": [
    "And at last, we see that the two *p*-values are essentially the same!\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f948c-07ce-4f19-91ba-ad1ee407ad20",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
