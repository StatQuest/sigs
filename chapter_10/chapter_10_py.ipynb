{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c33f29c-46e0-4328-beda-8f758684b57d",
   "metadata": {},
   "source": [
    "# [The StatQuest Illustrated Guide to Statistics](https://www.amazon.com/dp/B0GMP7Z9ZL)\n",
    "## Chapter 10 - *p*-value Pitfalls and How to Avoid Them!!!!!!\n",
    "\n",
    "Copyright 2026, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356cead-21bc-4a79-bfcc-972787453cab",
   "metadata": {},
   "source": [
    "In this notebook we'll learn how to...\n",
    "\n",
    "- Observe the Multiple Testing problem first hand.\n",
    "- Adjust *p*-values to counteract the effects of multiple testing.\n",
    "- Observe the negative affects of Significance Chasing.\n",
    "\n",
    "**NOTE:**\n",
    "This tutorial assumes that you have installed **[Python](https://www.python.org/)** and read Chapter 10 in **[The StatQuest Illustrated Guide to Statistics](https://www.amazon.com/dp/B0GMP7Z9ZL)**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c768e3-8368-4947-8580-d4c1df636ab8",
   "metadata": {},
   "source": [
    "Since we're using Python, the first thing we do is load in some modules that will help us do math and plot graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718cf0b-7b3d-4d23-b98a-3d2b680b24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to generate random numbers\n",
    "from scipy.stats import ttest_ind # to do t-tests\n",
    "import seaborn as sns # to draw a graphs and have them look somewhat nice\n",
    "from statsmodels.stats.multitest import multipletests # to adjust p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48cf0c-110f-4ccc-b6cc-ba6eb42c9463",
   "metadata": {},
   "source": [
    "# Observing the Multiple Testing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba69057-c140-4c45-89af-fb7e8fb76b5b",
   "metadata": {},
   "source": [
    "In order to observe the multiple testing problem, we're going to do a lot of *t*-tests where the null hypothesis is true, both sets of measurements come from the same distribution. We'll then save all of the *p*-values and see how many are less than 0.05, and thus, false positives. If things work as expected, we should get about 5% false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d3c84-ab71-41f3-a188-b1785730f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## since we're going to generate random datasets,\n",
    "## let's start by setting the seed so that the results\n",
    "## are reproducable\n",
    "np.random.seed(42)\n",
    "\n",
    "pop_mean = 0\n",
    "pop_sd = 1\n",
    "\n",
    "## Next, we define the number of random\n",
    "## datasets we wantt o create...\n",
    "num_rand_datasets = 10_000\n",
    "\n",
    "## ...and we define the number of data points\n",
    "## per dataset\n",
    "num_datapoints = 3\n",
    "\n",
    "## Create an empty array that is num.rand.datasets long\n",
    "p_values = np.empty(num_rand_datasets)\n",
    "\n",
    "## Here is the loop were we create a bunch of random datasets,\n",
    "## each with num.datapoints values, do a t-test and \n",
    "## then keep track of the corresponding p-value\n",
    "for i in range(num_rand_datasets):\n",
    "    group_a = np.random.normal(loc=pop_mean, scale=pop_sd, size=num_datapoints)\n",
    "    group_b = np.random.normal(loc=pop_mean, scale=pop_sd, size=num_datapoints)\n",
    "\n",
    "    ## t-test with equal variances = F-test\n",
    "    results = ttest_ind(group_a, group_b, equal_var=True)\n",
    "    p_values[i] = results.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c060077-5889-40cf-b47d-0fc77d47cbf5",
   "metadata": {},
   "source": [
    "Now let's draw a histogram of the *p*-values values with the `histplot()` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea4b58-27aa-44cb-a22a-e094d258d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f733a90-af1e-4f13-a2ab-678efc600600",
   "metadata": {},
   "source": [
    "The histogram shows that *p*-values are (relatively) uniformly distributed between 0 and 1. This makes sense if, for whatever threshold we want to use is, $x$, then we should get $x \\times 100$ false positives. For example, if the threshold is 0.05, like it is here, then we should get 5% false positives and thus, 5% of the *p*-values should be < 0.05. If, on the other hand, the threshold is 0.1, then we should get 10% false positvies, and thus, 10% of the *p*-values should be < 0.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773aae4-8f40-4ca6-9f97-504f9bac7240",
   "metadata": {},
   "source": [
    "Now let's calculate the number of false positives, the number of *p*-values < 0.05..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410c58f-b850-43d8-9a2c-32ee3e196c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_false_positives = np.sum(p_values < 0.05)\n",
    "num_false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bc83c-9f7a-4a4f-b4bc-bf2180357731",
   "metadata": {},
   "source": [
    "...and we see we got 501 false positives.\n",
    "\n",
    "Now calculate the percentage of false positives, the percentage of *p*-values < 0.05..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d85f3-dcc6-4a78-9ba4-2f67e3121bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_false_positives / num_rand_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1065c-aeb2-4f76-940e-112046056de4",
   "metadata": {},
   "source": [
    "...and we see that about 5% of the *p*-values resulted in false positives. So, things worked as expected and we got a bunch of false positives from multiple testing, even though we know that the Null Hypothesis is false.\n",
    "\n",
    "# Bummer!\n",
    "\n",
    "Now let's see what happens if we adjust those p-values to compensate for multiple testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf58b02-828a-4359-b50a-132c455c82a1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c8301-69b3-4bad-adff-e9085e74d424",
   "metadata": {},
   "source": [
    "# Adjusting *p*-values to compensate for multiple testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fe204-43bf-4717-a6db-8cb69ad61318",
   "metadata": {},
   "source": [
    "Since we created a bunch of *p*-values when we know that the Null Hypothesis is true, it should be interesting to see how many of 501 that were < 0.05 remain < 0.05 after adjusting them for multple testing. In this example, we'll start by using the Holm correction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3bee5-68bc-48f1-97b9-b5acbeb12431",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, adjust the p-values with the holm correction\n",
    "adjusted_stuff = multipletests(p_values, method='holm')\n",
    "\n",
    "## adjustd stuff is a tuple, and the 2nd thing is an array\n",
    "## of adjusted pvalues\n",
    "adjusted_stuff[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa110b-c645-419f-8592-f8767964eedb",
   "metadata": {},
   "source": [
    "...then we'll calculate the number of false positives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee792ba-827a-471c-a280-c63eb1aed9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now determine how many adjusted p-values are false positives\n",
    "num_false_positives_holm = np.sum(adjusted_stuff[1] < 0.05)\n",
    "\n",
    "num_false_positives_holm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7966b4-9d9f-4b70-8686-f6fb54cf23f9",
   "metadata": {},
   "source": [
    "...and we see that, after adjusting the *p*-values with the Holm correction, there are no longer any false positives.\n",
    "\n",
    "Now let's see if the same thing happens with using the False Discovery Rate, FDR, to adjust the *p*-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb717d-0d0e-436f-bd36-23d135939284",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_stuff = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "## adjustd stuff is a tuple, and the 2nd thing is an array\n",
    "## of adjusted pvalues\n",
    "adjusted_stuff[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f1524-1f45-43c0-b34c-4e9070997b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now determine how many adjusted p-values are false positives\n",
    "num_false_positives_holm = np.sum(adjusted_stuff[1] < 0.05)\n",
    "\n",
    "num_false_positives_holm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47369bb-c2ab-48bd-87b3-59a34d0e4884",
   "metadata": {},
   "source": [
    "So, either way we adjust the *p*-values, we eliminate all the false positives.\n",
    "\n",
    "# Double BAM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0cd4c-49b3-4eb7-8371-e040b2823f3f",
   "metadata": {},
   "source": [
    "Now let's observe the negative efffects of significance chasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f948c-07ce-4f19-91ba-ad1ee407ad20",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017d5b5-6593-4771-b111-c97bad649092",
   "metadata": {},
   "source": [
    "# Observing the negative effects of Significance Chasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140fd1b-0803-41bc-96a3-899d81b2e229",
   "metadata": {},
   "source": [
    "Just like we did earlier when we wanted to observed the multiple testing problem, here we're going to do a lot of *t*-tests where the null hypothesis is true, both sets of measurements come from the same distribution. However, this time we'll select tests where the *p*-values is close to 0.05, but still greater than it. For those tests, we'll then add one additional measurement per group and save the *p*-value with all the others. If adding the new data to the original tests that have *p*-values close to 0.05 does not result in additoinal false positives, we should have about 5% false positives in the end. If we have more than 5% false positives, then we will have seen the effects of significance chasing, an increase of false positives by adding data to tests that look promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833fb28-9b09-4e4f-8b41-a77441c03a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## since we're going to generate random datasets,\n",
    "## let's start by setting the seed so that the results\n",
    "## are reproducable\n",
    "np.random.seed(42)\n",
    "\n",
    "pop_mean <- 0\n",
    "pop_sd <- 1\n",
    "\n",
    "## Next, we define the number of random\n",
    "## datasets we wantt o create...\n",
    "num_rand_datasets <- 10_000\n",
    "\n",
    "## ...and we define the number of data points\n",
    "## per dataset\n",
    "num_datapoints <- 3\n",
    "\n",
    "## Create an empty array that is num.rand.datasets long\n",
    "p_values = np.empty(num_rand_datasets)\n",
    "\n",
    "## Here is the loop were we create a bunch of random datasets,\n",
    "## each with num.datapoints values, do a t-test and \n",
    "## then keep track of the corresponding p-value\n",
    "for i in range(num_rand_datasets):\n",
    "    \n",
    "    group_a = np.random.normal(loc=pop_mean, scale=pop_sd, size=num_datapoints)\n",
    "    group_b = np.random.normal(loc=pop_mean, scale=pop_sd, size=num_datapoints)\n",
    "\n",
    "    ## t-test with equal variances = F-test\n",
    "    results = ttest_ind(group_a, group_b, equal_var=True)\n",
    "\n",
    "    if 0.05 < results.pvalue < 0.08:\n",
    "        extra_a = np.random.normal(loc=pop_mean, scale=pop_sd, size=1)\n",
    "        extra_b = np.random.normal(loc=pop_mean, scale=pop_sd, size=1)\n",
    "\n",
    "        group_a = np.append(group_a, extra_a)\n",
    "        group_b = np.append(group_b, extra_b)\n",
    "\n",
    "        results = ttest_ind(group_a, group_b, equal_var=True)\n",
    "\n",
    "    p_values[i] = results.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4fd5e-ae19-42d9-9ab7-70911ee80c9a",
   "metadata": {},
   "source": [
    "Now let's draw a histogram ofthe *p*-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e87709-140c-4724-b49a-81491125215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e384f5f-db5e-4ca7-abb7-c3d252458dee",
   "metadata": {},
   "source": [
    "In the histogram, we can see that the second column has a lot fewer *p*-values in it than any of the other columns. This is because the *t*-tests associated with the *p*-values in that bin were re-done with additional data. As a result, we have more than 500 *p*-values in the surrounding columns, including the first column, which suggests that we could have more than 500 false positives. So let's count the number of false positives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387977a7-77c7-4d30-a5f3-2389a3b9ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_false_positives = np.sum(p_values < 0.05)\n",
    "num_false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36854988-d465-45f7-ad7c-9cb3da14265c",
   "metadata": {},
   "source": [
    "...and calculate the percentage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc6932-dd5d-42b8-b156-e1556cb7e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_false_positives / num_rand_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203bb1a8-a602-4a38-8a2e-03f9ea87ba43",
   "metadata": {},
   "source": [
    "...and we see that, compared to the original illustration of the multiple testing problem, when we both expected and received 5.0% of false positives, when we added data to promising *t*-tests, we ended up with 5.7% false positives, or an additional 51 false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7ecd3-804e-4cdf-ae95-aadcd868ec83",
   "metadata": {},
   "source": [
    "So, the moral of the story is, if a test looks promising, don't just add additional data existing measurements. Instead, do a proper power analysis and start over, collecting new data.\n",
    "\n",
    "# TRIPLE BUMMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ec318-c673-4428-ac31-3d29818c022f",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
